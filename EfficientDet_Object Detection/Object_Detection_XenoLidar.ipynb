{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from Object_Detection_XenoLidar.objectdetection.efficientnet.backbone import EfficientDetBackbone\n",
    "from Object_Detection_XenoLidar.objectdetection.efficientnet.efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from Object_Detection_XenoLidar.objectdetection.efficientnet.utils.utils import preprocess, invert_affine, postprocess, preprocess_video\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self) -> None:\n",
    "        # EfficientDet configuration\n",
    "\n",
    "        self.compound_coef = 7\n",
    "        self.force_input_size = None  # set None to use default size\n",
    "\n",
    "        self.threshold = 0.3\n",
    "        self.iou_threshold = 0.3\n",
    "\n",
    "        # Gets the GPU if there is one, otherwise the cpu\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.use_float16 = False\n",
    "        \n",
    "        cudnn.fastest = True\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        # list of object categories that can be detected by the AI object detection\n",
    "        self.obj_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "                    'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "                    'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n",
    "                    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "                    'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "                    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "                    'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
    "                    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "                    'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "                    'toothbrush']\n",
    "\n",
    "        # list of object categories allowed to be outputted\n",
    "        self.out_class_id_dict = {\n",
    "            'person'     : 1,\n",
    "            'bicycle'    : 2,\n",
    "            'motorcycle' : 3,\n",
    "            'car'        : 4,\n",
    "            'truck'      : 6,\n",
    "            'bus'        : 7\n",
    "        }\n",
    "        \n",
    "        # tf bilinear interpolation is different from any other's, just make do\n",
    "        self.input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "        self.input_size = self.input_sizes[self.compound_coef] if self.force_input_size is None else self.force_input_size\n",
    "\n",
    "        # load model\n",
    "        self.model = EfficientDetBackbone(compound_coef=self.compound_coef, num_classes=len(self.obj_list))\n",
    "        self.model.load_state_dict(torch.load(f'aiannotator/objectdetection/efficientnet/weights/efficientdet-d{self.compound_coef}.pth'))\n",
    "        self.model.requires_grad_(False)\n",
    "        self.model.eval()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        if self.use_float16:\n",
    "            self.model = self.model.half()\n",
    "\n",
    "        # Box\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "        self.clipBoxes    = ClipBoxes()\n",
    "\n",
    "    def Detect(self, image):\n",
    "\n",
    "        if image.shape[2] == 4:\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "        if image.shape[2] == 1:\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "        if image.shape[2] == 3:\n",
    "            rgb_image = image\n",
    "        \n",
    "        ori_imgs, framed_imgs, framed_metas = preprocess_video(rgb_image, max_size=self.input_size)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "        else:\n",
    "            x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "        x = x.to(torch.float32 if not self.use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "        # model predict\n",
    "        with torch.no_grad():\n",
    "            features, regression, classification, anchors = self.model(x)\n",
    "\n",
    "            out = postprocess(x,\n",
    "                            anchors, regression, classification,\n",
    "                            self.regressBoxes, self.clipBoxes,\n",
    "                            self.threshold, self.iou_threshold)\n",
    "\n",
    "        # result\n",
    "        out = invert_affine(framed_metas, out)\n",
    "\n",
    "        # function for display\n",
    "        def display(preds, imgs):\n",
    "            for i in range(len(imgs)):\n",
    "                if len(preds[i]['rois']) == 0:\n",
    "                    return imgs[i]\n",
    "\n",
    "                for j in range(len(preds[i]['rois'])):\n",
    "                    (x1, y1, x2, y2) = preds[i]['rois'][j].astype(np.int)\n",
    "                    cv2.rectangle(imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "                    obj = self.obj_list[preds[i]['class_ids'][j]]\n",
    "                    score = float(preds[i]['scores'][j])\n",
    "\n",
    "                    cv2.putText(imgs[i], '{}, {:.3f}'.format(obj, score),\n",
    "                                (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                (255, 255, 0), 1)\n",
    "                \n",
    "                return imgs[i]\n",
    "\n",
    "        def output_shapes(preds, imgs):\n",
    "            \"\"\" label, points \"\"\"\n",
    "            shapes = []\n",
    "            for i in range(len(imgs)):\n",
    "                if len(preds[i]['rois']) == 0:\n",
    "                    return shapes\n",
    "\n",
    "                for j in range(len(preds[i]['rois'])):\n",
    "                    (x_min, y_min, x_max, y_max) = preds[i]['rois'][j].astype(np.int)\n",
    "                    points = [(x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max)]\n",
    "                    label = self.obj_list[preds[i]['class_ids'][j]]\n",
    "\n",
    "                    if label in self.out_class_id_dict.keys():\n",
    "                        shapes.append((label, points, None, None, True))\n",
    "                \n",
    "                return shapes\n",
    "\n",
    "        # show frame by frame\n",
    "        shapes = output_shapes(out, ori_imgs)\n",
    "\n",
    "        return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator = ObjectDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import XenoWareFormat as xw\n",
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "def scale_image_base(image, ceil, floor):\n",
    "\n",
    "    a = 255/(ceil-floor)\n",
    "    b = floor*255/(floor-ceil)\n",
    "    out = np.maximum(0,np.minimum(255,image*a+b)).astype(np.uint8)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeToJSON(txtfile, file_name):\n",
    "    '''\n",
    "    txt_file - file path to XL annotation file\n",
    "    ptc_file - file path to the XPC point cloud file\n",
    "    '''\n",
    "    json_file = [{\n",
    "        \"image\"   : None,\n",
    "        \"verified\"   :False,\n",
    "        \"annotations\" : []\n",
    "       \n",
    "              }]\n",
    "    \n",
    "    #file = open(txtfile, \"r+\")\n",
    "    list_add = []\n",
    "    json_file[0][\"image\"]= file_name\n",
    "    #print(os.path.basename(txtfile))\n",
    "    \n",
    "    for idx, i in enumerate(txtfile):\n",
    "        annotation = {\"label\"       : None,\n",
    "                      \"coordinates\":{\n",
    "                                      'x':None,\n",
    "                                      'y':None,\n",
    "                                      'width':None,\n",
    "                                      'height':None\n",
    "                      }}\n",
    "        \n",
    "\n",
    "        [x1,y1] = i[1][0]\n",
    "        [x2,y2] = i[1][2]\n",
    "        \n",
    "        #class_id = int(i.split(\",\")[4].replace('\\n',\"\"))\n",
    "        annotation [\"label\"] = i[0]\n",
    "        annotation[\"coordinates\"]['x'] = int((x1+x2)/2)\n",
    "        annotation[\"coordinates\"]['y'] = int((y1+y2)/2)\n",
    "        annotation[\"coordinates\"]['width'] = int(x2-x1)\n",
    "        annotation[\"coordinates\"]['height'] = int(y2-y1)\n",
    "        \n",
    "        json_file [0][\"annotations\"].append(annotation)\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 329 visual images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "visual_image_dir = r'C:\\Users\\Adarsh Kuzhipathalil\\Applications_team\\Datasets\\XW_Output_Arvoo\\Parking Bodart\\VisualImages'\n",
    "visual_image_filenames = glob.glob(os.path.join(visual_image_dir,'xl_visual*.xim'))\n",
    "visual_image_filenames.sort()\n",
    "print('Found {} visual images'.format(len(visual_image_filenames)))\n",
    "visual_image_filenames[0].split('/')[-1].replace('.vis','.txt')\n",
    "out_dir = r'C:\\Users\\Adarsh Kuzhipathalil\\Applications_team\\Datasets\\XW_Output_Arvoo\\Parking Bodart\\Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/329 [00:00<?, ?it/s]C:\\Users\\ADARSH~1\\AppData\\Local\\Temp/ipykernel_26524/3446600888.py:123: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (x_min, y_min, x_max, y_max) = preds[i]['rois'][j].astype(np.int)\n",
      "100%|██████████| 329/329 [05:13<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for visual_image_filename in tqdm.tqdm(visual_image_filenames):\n",
    "\n",
    "    [c1,_] = xw.XW_ReadFile(visual_image_filename) \n",
    "    visual_image = c1['data'] \n",
    "\n",
    "    frame = scale_image_base(visual_image,120,0)\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "    shapes = Annotator.Detect(frame)\n",
    "    json_annotations = changeToJSON(shapes,os.path.basename(visual_image_filename))\n",
    "    out_file = os.path.join(out_dir, os.path.basename(visual_image_filename).replace('.xim','.json'))\n",
    "    with open(out_file, 'w') as outfile:\n",
    "        json.dump(json_annotations, outfile,ensure_ascii=False, indent=2)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': 'xl_visual00004292.xim',\n",
       "  'verified': False,\n",
       "  'annotations': [{'label': 'car',\n",
       "    'coordinates': {'x': 1015.5, 'y': 69.5, 'width': 143, 'height': 45}}]}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xl_visual00004292.xim'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.basename(visual_image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abeae5f0bdba8b302eff936e3cf43682d098772429d74ace23a38ed451366731"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
